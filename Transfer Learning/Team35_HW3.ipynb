{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jean/tensorflow/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "# to make sure the graph is refresh\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# load data: digits 5 to 9, but still label with 0 to 4, \n",
    "# because TensorFlow expects label's integers from 0 to n_classes-1.\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")\n",
    "\n",
    "X_train2_full = mnist.train.images[mnist.train.labels >= 5]\n",
    "y_train2_full = mnist.train.labels[mnist.train.labels >= 5] - 5\n",
    "X_valid2_full = mnist.validation.images[mnist.validation.labels >= 5]\n",
    "y_valid2_full = mnist.validation.labels[mnist.validation.labels >= 5] - 5\n",
    "X_test2 = mnist.test.images[mnist.test.labels >= 5]\n",
    "y_test2 = mnist.test.labels[mnist.test.labels >= 5] - 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want to keep only 100 instances per class in the training set \n",
    "# and let's keep only 30 instances per class in the validation set\n",
    "# tesing set is already loaded above\n",
    "def sample_n_instances_per_class(X, y, n=100):\n",
    "    Xs, ys = [], []\n",
    "    for label in np.unique(y):\n",
    "        idx = (y == label)\n",
    "        Xc = X[idx][:n]\n",
    "        yc = y[idx][:n]\n",
    "        Xs.append(Xc)\n",
    "        ys.append(yc)\n",
    "    return np.concatenate(Xs), np.concatenate(ys)\n",
    "\n",
    "X_train2, y_train2 = sample_n_instances_per_class(X_train2_full, y_train2_full, n=100)\n",
    "X_valid2, y_valid2 = sample_n_instances_per_class(X_valid2_full, y_valid2_full, n=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one_hot encoding 5, 6, 7, 8, 9 for all labels\n",
    "def one_hot_encoding(y):\n",
    "    tmp_y = np.zeros([y.shape[0], 5])\n",
    "    for i in range(y.shape[0]):\n",
    "        tmp_y[i][y[i]] = 1\n",
    "    return tmp_y\n",
    "\n",
    "y_train2 = one_hot_encoding(y_train2)\n",
    "y_valid2 = one_hot_encoding(y_valid2)\n",
    "y_test2 = one_hot_encoding(y_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyper-parameters for all networks structures\n",
    "learning_rate = 0.01\n",
    "batch_size = 32\n",
    "epoch_bound = 1000\n",
    "stop_threshold = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HW3-1: Softmax Only\n",
    "reset_graph()\n",
    "pretrained_model_path = \"./saved_model/Team35_HW2\"\n",
    "new_model_path = \"./saved_model/Team35_HW3_1\"\n",
    "# get graph from pretrained model\n",
    "pretrained_saver = tf.train.import_meta_graph(pretrained_model_path + \".ckpt.meta\")\n",
    "# new saver for HW3-1\n",
    "new_saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get variables from pretrained model\n",
    "X = tf.get_default_graph().get_tensor_by_name(\"X:0\")\n",
    "y = tf.get_default_graph().get_tensor_by_name(\"y:0\")\n",
    "loss = tf.get_default_graph().get_tensor_by_name(\"loss:0\")\n",
    "accuracy = tf.get_default_graph().get_tensor_by_name(\"accuracy:0\")\n",
    "\n",
    "# create new training layers\n",
    "output_layer_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"logits\")\n",
    "\n",
    "# define new Adam optimizer and training steps\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate, name=\"AdamOp_3-1\")\n",
    "training_op = optimizer.minimize(loss, var_list=output_layer_vars, name=\"training_op_3-1\")\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_only_softmax(X_train, y_train, X_validate, y_validate, train_op, epoch_bound, stop_threshold, batch_size, testing=False, new_saver=None, new_model_path=None):\n",
    "    \n",
    "    early_stop = 0\n",
    "    winner_loss = np.infty\n",
    "    winner_accuracy = 0.0\n",
    "    \n",
    "    t0 = time.time()\n",
    "    \n",
    "    for epoch in range(epoch_bound):\n",
    "\n",
    "        # randomize training set\n",
    "        indices_training = np.random.permutation(X_train.shape[0])\n",
    "        X_train, y_train = X_train[indices_training,:], y_train[indices_training,:]\n",
    "\n",
    "        # split training set into multiple mini-batches and start training\n",
    "        total_batches = int(X_train.shape[0] / batch_size)\n",
    "        for batch in range(total_batches):\n",
    "            if batch == total_batches - 1:\n",
    "                sess.run(train_op, feed_dict={X: X_train[batch*batch_size:], y: y_train[batch*batch_size:]})\n",
    "            else:\n",
    "                sess.run(train_op, feed_dict={X: X_train[batch*batch_size : (batch+1)*batch_size], y: y_train[batch*batch_size : (batch+1)*batch_size]})\n",
    "\n",
    "        # compute validation accuracy\n",
    "        cur_accuracy, cur_loss = evaluate_with_only_softmax(X_validate, y_validate)\n",
    "\n",
    "        # If the accuracy rate does not increase for many times, it will early stop epochs-loop \n",
    "        if winner_loss > cur_loss:\n",
    "            early_stop = 0\n",
    "            winner_loss = cur_loss\n",
    "            winner_accuracy = cur_accuracy\n",
    "            # save best model in testing phase\n",
    "            if testing == True:\n",
    "                save_path = new_saver.save(sess, new_model_path + \".ckpt\")\n",
    "        else:\n",
    "            early_stop += 1\n",
    "        print(\"{}\\tValidation loss: {:.6f}\\tBest loss: {:.6f}\\tAccuracy: {:.2f}%\".format(epoch, cur_loss, winner_loss, cur_accuracy * 100))\n",
    "        if early_stop == stop_threshold:\n",
    "            print(\"Early Stop.\")\n",
    "            break\n",
    "    t1 = time.time()\n",
    "    print(\"Total training time of HW3-1: {:.1f}s\".format(t1 - t0))\n",
    "    \n",
    "    return winner_accuracy, winner_loss\n",
    "\n",
    "# evaluate model: compute accuracy and loss\n",
    "def evaluate_with_only_softmax(Inputs, Labels):\n",
    "    global accuracy, loss\n",
    "    acc = sess.run(accuracy, feed_dict={X: Inputs, y:Labels})\n",
    "    loss_val = sess.run(loss, feed_dict={X: Inputs, y:Labels})        \n",
    "    return acc, loss_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\tValidation loss: 0.858134\tBest loss: 0.858134\tAccuracy: 94.46%\n",
      "1\tValidation loss: 0.715841\tBest loss: 0.715841\tAccuracy: 94.46%\n",
      "2\tValidation loss: 0.602619\tBest loss: 0.602619\tAccuracy: 94.46%\n",
      "3\tValidation loss: 0.579836\tBest loss: 0.579836\tAccuracy: 94.46%\n",
      "4\tValidation loss: 0.562980\tBest loss: 0.562980\tAccuracy: 94.46%\n",
      "5\tValidation loss: 0.555397\tBest loss: 0.555397\tAccuracy: 94.46%\n",
      "6\tValidation loss: 0.580309\tBest loss: 0.555397\tAccuracy: 94.46%\n",
      "7\tValidation loss: 0.540856\tBest loss: 0.540856\tAccuracy: 94.46%\n",
      "8\tValidation loss: 0.518206\tBest loss: 0.518206\tAccuracy: 94.46%\n",
      "9\tValidation loss: 0.524833\tBest loss: 0.518206\tAccuracy: 94.46%\n",
      "10\tValidation loss: 0.513265\tBest loss: 0.513265\tAccuracy: 94.46%\n",
      "11\tValidation loss: 0.525739\tBest loss: 0.513265\tAccuracy: 94.46%\n",
      "12\tValidation loss: 0.516617\tBest loss: 0.513265\tAccuracy: 94.46%\n",
      "13\tValidation loss: 0.505502\tBest loss: 0.505502\tAccuracy: 94.46%\n",
      "14\tValidation loss: 0.519397\tBest loss: 0.505502\tAccuracy: 94.46%\n",
      "15\tValidation loss: 0.513783\tBest loss: 0.505502\tAccuracy: 94.46%\n",
      "16\tValidation loss: 0.486136\tBest loss: 0.486136\tAccuracy: 94.46%\n",
      "17\tValidation loss: 0.517711\tBest loss: 0.486136\tAccuracy: 94.46%\n",
      "18\tValidation loss: 0.491616\tBest loss: 0.486136\tAccuracy: 94.46%\n",
      "19\tValidation loss: 0.499259\tBest loss: 0.486136\tAccuracy: 94.46%\n",
      "20\tValidation loss: 0.483676\tBest loss: 0.483676\tAccuracy: 94.46%\n",
      "21\tValidation loss: 0.506643\tBest loss: 0.483676\tAccuracy: 94.46%\n",
      "22\tValidation loss: 0.506268\tBest loss: 0.483676\tAccuracy: 94.46%\n",
      "23\tValidation loss: 0.490043\tBest loss: 0.483676\tAccuracy: 94.46%\n",
      "24\tValidation loss: 0.484263\tBest loss: 0.483676\tAccuracy: 94.46%\n",
      "25\tValidation loss: 0.502927\tBest loss: 0.483676\tAccuracy: 94.46%\n",
      "26\tValidation loss: 0.481214\tBest loss: 0.481214\tAccuracy: 94.46%\n",
      "27\tValidation loss: 0.513007\tBest loss: 0.481214\tAccuracy: 94.46%\n",
      "28\tValidation loss: 0.504546\tBest loss: 0.481214\tAccuracy: 94.46%\n",
      "29\tValidation loss: 0.491189\tBest loss: 0.481214\tAccuracy: 94.46%\n",
      "30\tValidation loss: 0.508332\tBest loss: 0.481214\tAccuracy: 94.46%\n",
      "31\tValidation loss: 0.485223\tBest loss: 0.481214\tAccuracy: 94.46%\n",
      "32\tValidation loss: 0.486450\tBest loss: 0.481214\tAccuracy: 94.46%\n",
      "33\tValidation loss: 0.527437\tBest loss: 0.481214\tAccuracy: 94.46%\n",
      "34\tValidation loss: 0.484546\tBest loss: 0.481214\tAccuracy: 94.46%\n",
      "35\tValidation loss: 0.501023\tBest loss: 0.481214\tAccuracy: 94.46%\n",
      "36\tValidation loss: 0.512260\tBest loss: 0.481214\tAccuracy: 94.46%\n",
      "37\tValidation loss: 0.510601\tBest loss: 0.481214\tAccuracy: 94.46%\n",
      "38\tValidation loss: 0.504381\tBest loss: 0.481214\tAccuracy: 94.46%\n",
      "39\tValidation loss: 0.482483\tBest loss: 0.481214\tAccuracy: 94.46%\n",
      "40\tValidation loss: 0.533124\tBest loss: 0.481214\tAccuracy: 94.46%\n",
      "41\tValidation loss: 0.491381\tBest loss: 0.481214\tAccuracy: 94.46%\n",
      "42\tValidation loss: 0.480276\tBest loss: 0.480276\tAccuracy: 94.46%\n",
      "43\tValidation loss: 0.539119\tBest loss: 0.480276\tAccuracy: 94.46%\n",
      "44\tValidation loss: 0.498901\tBest loss: 0.480276\tAccuracy: 94.46%\n",
      "45\tValidation loss: 0.512628\tBest loss: 0.480276\tAccuracy: 94.46%\n",
      "46\tValidation loss: 0.505525\tBest loss: 0.480276\tAccuracy: 94.46%\n",
      "47\tValidation loss: 0.524787\tBest loss: 0.480276\tAccuracy: 94.46%\n",
      "48\tValidation loss: 0.510310\tBest loss: 0.480276\tAccuracy: 94.46%\n",
      "49\tValidation loss: 0.505351\tBest loss: 0.480276\tAccuracy: 94.46%\n",
      "50\tValidation loss: 0.508504\tBest loss: 0.480276\tAccuracy: 94.46%\n",
      "51\tValidation loss: 0.523230\tBest loss: 0.480276\tAccuracy: 94.46%\n",
      "52\tValidation loss: 0.502402\tBest loss: 0.480276\tAccuracy: 94.46%\n",
      "53\tValidation loss: 0.547829\tBest loss: 0.480276\tAccuracy: 94.46%\n",
      "54\tValidation loss: 0.486923\tBest loss: 0.480276\tAccuracy: 94.46%\n",
      "55\tValidation loss: 0.522744\tBest loss: 0.480276\tAccuracy: 94.46%\n",
      "56\tValidation loss: 0.500740\tBest loss: 0.480276\tAccuracy: 94.46%\n",
      "57\tValidation loss: 0.545813\tBest loss: 0.480276\tAccuracy: 94.46%\n",
      "58\tValidation loss: 0.512872\tBest loss: 0.480276\tAccuracy: 94.46%\n",
      "59\tValidation loss: 0.513162\tBest loss: 0.480276\tAccuracy: 94.46%\n",
      "60\tValidation loss: 0.528109\tBest loss: 0.480276\tAccuracy: 94.46%\n",
      "61\tValidation loss: 0.527140\tBest loss: 0.480276\tAccuracy: 94.46%\n",
      "62\tValidation loss: 0.504463\tBest loss: 0.480276\tAccuracy: 94.46%\n",
      "Early Stop.\n",
      "Total training time of HW3-1: 229.6s\n",
      "INFO:tensorflow:Restoring parameters from ./saved_model/Team35_HW3_1.ckpt\n",
      "Test accuracy: 94.46%\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    # init weights\n",
    "    sess.run(init)\n",
    "    \n",
    "    # restore value from pretrained model\n",
    "    pretrained_saver.restore = (sess, pretrained_model_path + \".ckpt\")\n",
    "\n",
    "    # initialize value for softmax layer\n",
    "    for var in output_layer_vars:\n",
    "        sess.run(var.initializer)\n",
    "        \n",
    "    # training phase\n",
    "    winner_accuracy, winner_loss = train_with_only_softmax(X_train2, y_train2, X_valid2, y_valid2, training_op, epoch_bound, stop_threshold, batch_size, testing=True, new_saver=new_saver, new_model_path=new_model_path)\n",
    "\n",
    "    # testing phase\n",
    "    new_saver.restore(sess, new_model_path + \".ckpt\")\n",
    "    test_accuracy, test_loss = evaluate_with_only_softmax(X_test2, y_test2)\n",
    "    print(\"Test accuracy: {:.2f}%\".format(test_accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# HW3-2: cache 5th hidden layer\n",
    "# use the function, tf.get_default_graph().get_operations(), to find right tensor name\n",
    "# print(tf.get_default_graph().get_operations())\n",
    "reset_graph()\n",
    "pretrained_model_path = \"./saved_model/Team35_HW2\"\n",
    "new_model_path = \"./saved_model/Team35_HW3_2\"\n",
    "# get graph from pretrained model\n",
    "pretrained_saver = tf.train.import_meta_graph(pretrained_model_path + \".ckpt.meta\")\n",
    "# new saver for HW3-2\n",
    "new_saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get variables from pretrained model\n",
    "# print(tf.get_default_graph().get_operations())\n",
    "X = tf.get_default_graph().get_tensor_by_name(\"X:0\")\n",
    "y = tf.get_default_graph().get_tensor_by_name(\"y:0\")\n",
    "loss = tf.get_default_graph().get_tensor_by_name(\"loss:0\")\n",
    "accuracy = tf.get_default_graph().get_tensor_by_name(\"accuracy:0\")\n",
    "# get 5th hidden layer tensor\n",
    "h5_out = tf.get_default_graph().get_tensor_by_name(\"dnn_h5/Elu:0\")\n",
    "\n",
    "# create new training layers\n",
    "output_layer_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"logits\")\n",
    "\n",
    "# define new Adam optimizer and training steps\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate, name=\"AdamOp_3-2\")\n",
    "training_op = optimizer.minimize(loss, var_list=output_layer_vars, name=\"training_op_3-2\")\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_softmax_and_cache_h5(X_train, y_train, X_validate, y_validate, train_op, epoch_bound, stop_threshold, batch_size, testing=False, new_saver=None, new_model_path=None):\n",
    "        \n",
    "    early_stop = 0\n",
    "    winner_loss = np.infty\n",
    "    winner_accuracy = 0.0\n",
    "    \n",
    "    t0 = time.time()\n",
    "    \n",
    "    for epoch in range(epoch_bound):\n",
    "\n",
    "        # randomize training set\n",
    "        indices_training = np.random.permutation(X_train.shape[0])\n",
    "        X_train, y_train = X_train[indices_training,:], y_train[indices_training,:]\n",
    "\n",
    "        # split training set into multiple mini-batches and start training\n",
    "        total_batches = int(X_train.shape[0] / batch_size)\n",
    "        for batch in range(total_batches):\n",
    "            if batch == total_batches - 1:\n",
    "                sess.run(train_op, feed_dict={h5_out: X_train[batch*batch_size:], y: y_train[batch*batch_size:]})\n",
    "            else:\n",
    "                sess.run(train_op, feed_dict={h5_out: X_train[batch*batch_size : (batch+1)*batch_size], y: y_train[batch*batch_size : (batch+1)*batch_size]})\n",
    "\n",
    "        # compute validation accuracy\n",
    "        cur_accuracy, cur_loss = evaluate_with_softmax_and_cache_h5(X_validate, y_validate)\n",
    "\n",
    "        # If the accuracy rate does not increase for many times, it will early stop epochs-loop \n",
    "        if winner_loss > cur_loss:\n",
    "            early_stop = 0\n",
    "            winner_loss = cur_loss\n",
    "            winner_accuracy = cur_accuracy\n",
    "            # save best model in testing phase\n",
    "            if testing == True:\n",
    "                save_path = new_saver.save(sess, new_model_path + \".ckpt\")\n",
    "        else:\n",
    "            early_stop += 1\n",
    "        print(\"{}\\tValidation loss: {:.6f}\\tBest loss: {:.6f}\\tAccuracy: {:.2f}%\".format(epoch, cur_loss, winner_loss, cur_accuracy * 100))\n",
    "        if early_stop == stop_threshold:\n",
    "            print(\"Early Stop.\")\n",
    "            break\n",
    "    t1 = time.time()\n",
    "    print(\"Total training time of HW3-2: {:.1f}s\".format(t1 - t0))\n",
    "    \n",
    "    return winner_accuracy, winner_loss\n",
    "\n",
    "# evaluate model: compute accuracy and loss\n",
    "def evaluate_with_softmax_and_cache_h5(Inputs, Labels):\n",
    "    global accuracy, loss\n",
    "    acc = sess.run(accuracy, feed_dict={h5_out: Inputs, y:Labels})\n",
    "    loss_val = sess.run(loss, feed_dict={h5_out: Inputs, y:Labels})        \n",
    "    return acc, loss_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    # init weights\n",
    "    sess.run(init)\n",
    "    \n",
    "    # restore value from pretrained model\n",
    "    pretrained_saver.restore = (sess, pretrained_model_path + \".ckpt\")\n",
    "\n",
    "    # initialize value for softmax layer\n",
    "    for var in output_layer_vars:\n",
    "        sess.run(var.initializer)\n",
    "        \n",
    "    # Feed training set and validation set into 5th layer\n",
    "    h5_train = sess.run(h5_out, feed_dict={X: X_train2, y: y_train2})\n",
    "    h5_valid = sess.run(h5_out, feed_dict={X: X_valid2, y: y_valid2})\n",
    "\n",
    "    # training phase\n",
    "    winner_accuracy, winner_loss = train_with_softmax_and_cache_h5(h5_train, y_train2, h5_valid, y_valid2, training_op, epoch_bound, stop_threshold, batch_size, testing=True, new_saver=new_saver, new_model_path=new_model_path)\n",
    "\n",
    "    # testing phase\n",
    "    new_saver.restore(sess, new_model_path + \".ckpt\")\n",
    "    test_accuracy, test_loss = evaluate_with_only_softmax(X_test2, y_test2)\n",
    "    print(\"Test accuracy: {:.2f}%\".format(test_accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HW3-3: 4 hidden layers instead and create new softmax layer\n",
    "reset_graph()\n",
    "pretrained_model_path = \"./saved_model/Team35_HW2\"\n",
    "new_model_path = \"./saved_model/Team35_HW3_3\"\n",
    "# get graph from pretrained model\n",
    "pretrained_saver = tf.train.import_meta_graph(pretrained_model_path + \".ckpt.meta\")\n",
    "# new saver for HW3-3\n",
    "new_saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get graph for 1~4 layers (transfer layers) \n",
    "X = tf.get_default_graph().get_tensor_by_name(\"X:0\")\n",
    "y = tf.get_default_graph().get_tensor_by_name(\"y:0\")\n",
    "\n",
    "# get 4th hidden layer tensor\n",
    "h4_out = tf.get_default_graph().get_tensor_by_name(\"dnn_h4/Elu:0\")\n",
    "\n",
    "# create new training layers\n",
    "# add outputs softmax layer with 5 neurals\n",
    "logits = tf.layers.dense(inputs=h4_out, units=5, kernel_initializer=tf.contrib.layers.variance_scaling_initializer(), name=\"logits_3-3\")\n",
    "outputs = tf.nn.softmax(logits, name=\"Y_probability_3-3\")\n",
    "output_layer_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"logits_3-3\")\n",
    "\n",
    "# cross entropy loss function\n",
    "cross_entropy_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y), name=\"loss_3-3\")\n",
    "\n",
    "# accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(outputs, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name=\"accuracy_3-3\")\n",
    "\n",
    "# training iteration and define Adam optimizer with learning rate\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate, name=\"AdamOp_3-3\")\n",
    "training_op = optimizer.minimize(cross_entropy_loss, var_list=output_layer_vars, name=\"training_op_3-3\")\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_4_hidden_and_new_softmax(X_train, y_train, X_validate, y_validate, train_op, epoch_bound, stop_threshold, batch_size, testing=False, new_saver=None, new_model_path=None):\n",
    "    \n",
    "    early_stop = 0\n",
    "    winner_loss = np.infty\n",
    "    winner_accuracy = 0.0\n",
    "    \n",
    "    t0 = time.time()\n",
    "    \n",
    "    for epoch in range(epoch_bound):\n",
    "\n",
    "        # randomize training set\n",
    "        indices_training = np.random.permutation(X_train.shape[0])\n",
    "        X_train, y_train = X_train[indices_training,:], y_train[indices_training,:]\n",
    "\n",
    "        # split training set into multiple mini-batches and start training\n",
    "        total_batches = int(X_train.shape[0] / batch_size)\n",
    "        for batch in range(total_batches):\n",
    "            if batch == total_batches - 1:\n",
    "                sess.run(train_op, feed_dict={X: X_train[batch*batch_size:], y: y_train[batch*batch_size:]})\n",
    "            else:\n",
    "                sess.run(train_op, feed_dict={X: X_train[batch*batch_size : (batch+1)*batch_size], y: y_train[batch*batch_size : (batch+1)*batch_size]})\n",
    "\n",
    "        # compute validation accuracy\n",
    "        cur_accuracy, cur_loss = evaluate_with_4_hidden_and_new_softmax(X_validate, y_validate)\n",
    "\n",
    "        # If the accuracy rate does not increase for many times, it will early stop epochs-loop \n",
    "        if winner_loss > cur_loss:\n",
    "            early_stop = 0\n",
    "            winner_loss = cur_loss\n",
    "            winner_accuracy = cur_accuracy\n",
    "            # save best model in testing phase\n",
    "            if testing == True:\n",
    "                save_path = new_saver.save(sess, new_model_path + \".ckpt\")\n",
    "        else:\n",
    "            early_stop += 1\n",
    "        print(\"{}\\tValidation loss: {:.6f}\\tBest loss: {:.6f}\\tAccuracy: {:.2f}%\".format(epoch, cur_loss, winner_loss, cur_accuracy * 100))\n",
    "        if early_stop == stop_threshold:\n",
    "            print(\"Early Stop.\")\n",
    "            break\n",
    "    t1 = time.time()\n",
    "    print(\"Total training time of HW3-3: {:.1f}s\".format(t1 - t0))\n",
    "    \n",
    "    return winner_accuracy, winner_loss\n",
    "\n",
    "# evaluate model: compute accuracy, precision, recall\n",
    "def evaluate_with_4_hidden_and_new_softmax(Inputs, Labels):\n",
    "    global accuracy, cross_entropy_loss\n",
    "    acc = sess.run(accuracy, feed_dict={X: Inputs, y:Labels})\n",
    "    loss_val = sess.run(cross_entropy_loss, feed_dict={X: Inputs, y:Labels})\n",
    "    return acc, loss_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    # init weights\n",
    "    sess.run(init)\n",
    "    \n",
    "    # restore value from pretrained model\n",
    "    pretrained_saver.restore = (sess, pretrained_model_path + \".ckpt\")\n",
    "\n",
    "    # initialize value for softmax layer\n",
    "    for var in output_layer_vars:\n",
    "        sess.run(var.initializer)\n",
    "    \n",
    "    # training phase\n",
    "    winner_accuracy, winner_loss = train_with_4_hidden_and_new_softmax(X_train2, y_train2, X_valid2, y_valid2, training_op, epoch_bound, stop_threshold, batch_size, testing=True, new_saver=new_saver, new_model_path=new_model_path)\n",
    "\n",
    "    # testing phase\n",
    "    new_saver.restore(sess, new_model_path + \".ckpt\")\n",
    "    test_accuracy, test_loss = evaluate_with_4_hidden_and_new_softmax(X_test2, y_test2)\n",
    "    print(\"Test accuracy: {:.2f}%\".format(test_accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HW3-4: unfreeze the top two hidden layers (1st and 2nd hidden layers) and continue training\n",
    "reset_graph()\n",
    "pretrained_model_path = \"./saved_model/Team35_HW2\"\n",
    "new_model_path = \"./saved_model/Team35_HW3_4\"\n",
    "# get graph from pretrained model\n",
    "pretrained_saver = tf.train.import_meta_graph(pretrained_model_path + \".ckpt.meta\")\n",
    "# new saver for HW3-4\n",
    "new_saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get graph for 1~4 layers (transfer layers) \n",
    "X = tf.get_default_graph().get_tensor_by_name(\"X:0\")\n",
    "y = tf.get_default_graph().get_tensor_by_name(\"y:0\")\n",
    "\n",
    "# get 4th hidden layer tensor\n",
    "h4_out = tf.get_default_graph().get_tensor_by_name(\"dnn_h4/Elu:0\")\n",
    "\n",
    "# create new training layers\n",
    "# add outputs softmax layer with 5 neurals\n",
    "logits = tf.layers.dense(inputs=h4_out, units=5, kernel_initializer=tf.contrib.layers.variance_scaling_initializer(), name=\"logits_3-4\")\n",
    "outputs = tf.nn.softmax(logits, name=\"Y_probability_3-4\")\n",
    "\n",
    "unfrozen_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"dnn_h1|dnn_h2|logits_3-4\")\n",
    "\n",
    "# cross entropy loss function\n",
    "cross_entropy_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y), name=\"loss_3-4\")\n",
    "\n",
    "# accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(outputs, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name=\"accuracy_3-4\")\n",
    "\n",
    "# training iteration and define Adam optimizer with learning rate\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate, name=\"AdamOp_3-4\")\n",
    "training_op = optimizer.minimize(cross_entropy_loss, var_list=unfrozen_vars, name=\"training_op_3-4\")\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_top_2_hidden_and_new_softmax(X_train, y_train, X_validate, y_validate, train_op, epoch_bound, stop_threshold, batch_size, testing=False, new_saver=None, new_model_path=None):\n",
    "    \n",
    "    early_stop = 0\n",
    "    winner_loss = np.infty\n",
    "    winner_accuracy = 0.0\n",
    "    \n",
    "    t0 = time.time()\n",
    "    \n",
    "    for epoch in range(epoch_bound):\n",
    "\n",
    "        # randomize training set\n",
    "        indices_training = np.random.permutation(X_train.shape[0])\n",
    "        X_train, y_train = X_train[indices_training,:], y_train[indices_training,:]\n",
    "\n",
    "        # split training set into multiple mini-batches and start training\n",
    "        total_batches = int(X_train.shape[0] / batch_size)\n",
    "        for batch in range(total_batches):\n",
    "            if batch == total_batches - 1:\n",
    "                sess.run(train_op, feed_dict={X: X_train[batch*batch_size:], y: y_train[batch*batch_size:]})\n",
    "            else:\n",
    "                sess.run(train_op, feed_dict={X: X_train[batch*batch_size : (batch+1)*batch_size], y: y_train[batch*batch_size : (batch+1)*batch_size]})\n",
    "\n",
    "        # compute validation accuracy\n",
    "        cur_accuracy, cur_loss = evaluate_with_top_2_hidden_and_new_softmax(X_validate, y_validate)\n",
    "\n",
    "        # If the accuracy rate does not increase for many times, it will early stop epochs-loop \n",
    "        if winner_loss > cur_loss:\n",
    "            early_stop = 0\n",
    "            winner_loss = cur_loss\n",
    "            winner_accuracy = cur_accuracy\n",
    "            # save best model in testing phase\n",
    "            if testing == True:\n",
    "                save_path = new_saver.save(sess, new_model_path + \".ckpt\")\n",
    "        else:\n",
    "            early_stop += 1\n",
    "        print(\"{}\\tValidation loss: {:.6f}\\tBest loss: {:.6f}\\tAccuracy: {:.2f}%\".format(epoch, cur_loss, winner_loss, cur_accuracy * 100))\n",
    "        if early_stop == stop_threshold:\n",
    "            print(\"Early Stop.\")\n",
    "            break\n",
    "    t1 = time.time()\n",
    "    print(\"Total training time of HW3-4: {:.1f}s\".format(t1 - t0))\n",
    "    \n",
    "    return winner_accuracy, winner_loss\n",
    "\n",
    "# evaluate model: compute accuracy, precision, recall\n",
    "def evaluate_with_top_2_hidden_and_new_softmax(Inputs, Labels):\n",
    "    global accuracy, cross_entropy_loss\n",
    "    acc = sess.run(accuracy, feed_dict={X: Inputs, y:Labels})\n",
    "    loss_val = sess.run(cross_entropy_loss, feed_dict={X: Inputs, y:Labels})\n",
    "    return acc, loss_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    # init weights\n",
    "    sess.run(init)\n",
    "    \n",
    "    # restore value from pretrained model\n",
    "    pretrained_saver.restore = (sess, pretrained_model_path + \".ckpt\")\n",
    "\n",
    "    # initialize value for softmax layer\n",
    "    for var in unfrozen_vars:\n",
    "        sess.run(var.initializer)\n",
    "    \n",
    "    # training phase\n",
    "    winner_accuracy, winner_loss = train_with_top_2_hidden_and_new_softmax(X_train2, y_train2, X_valid2, y_valid2, training_op, epoch_bound, stop_threshold, batch_size, testing=True, new_saver=new_saver, new_model_path=new_model_path)\n",
    "\n",
    "    # testing phase\n",
    "    new_saver.restore(sess, new_model_path + \".ckpt\")\n",
    "    test_accuracy, test_loss = evaluate_with_top_2_hidden_and_new_softmax(X_test2, y_test2)\n",
    "    print(\"Test accuracy: {:.2f}%\".format(test_accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-virtualenv-tensorflow",
   "language": "python",
   "name": "my-virtualenv-tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
