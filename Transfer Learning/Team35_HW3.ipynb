{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "# to make sure the graph is refresh\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "reset_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# load data: digits 5 to 9, but still label with 0 to 4, \n",
    "# because TensorFlow expects label's integers from 0 to n_classes-1.\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")\n",
    "\n",
    "X_train2_full = mnist.train.images[mnist.train.labels >= 5]\n",
    "y_train2_full = mnist.train.labels[mnist.train.labels >= 5] - 5\n",
    "X_valid2_full = mnist.validation.images[mnist.validation.labels >= 5]\n",
    "y_valid2_full = mnist.validation.labels[mnist.validation.labels >= 5] - 5\n",
    "X_test2 = mnist.test.images[mnist.test.labels >= 5]\n",
    "y_test2 = mnist.test.labels[mnist.test.labels >= 5] - 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want to keep only 100 instances per class in the training set \n",
    "# and let's keep only 30 instances per class in the validation set\n",
    "# tesing set is already loaded above\n",
    "def sample_n_instances_per_class(X, y, n=100):\n",
    "    Xs, ys = [], []\n",
    "    for label in np.unique(y):\n",
    "        idx = (y == label)\n",
    "        Xc = X[idx][:n]\n",
    "        yc = y[idx][:n]\n",
    "        Xs.append(Xc)\n",
    "        ys.append(yc)\n",
    "    return np.concatenate(Xs), np.concatenate(ys)\n",
    "\n",
    "X_train2, y_train2 = sample_n_instances_per_class(X_train2_full, y_train2_full, n=100)\n",
    "X_valid2, y_valid2 = sample_n_instances_per_class(X_valid2_full, y_valid2_full, n=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one_hot encoding 5, 6, 7, 8, 9 for all labels\n",
    "def one_hot_encoding(y):\n",
    "    tmp_y = np.zeros([y.shape[0], 5])\n",
    "    for i in range(y.shape[0]):\n",
    "        tmp_y[i][y[i]] = 1\n",
    "    return tmp_y\n",
    "\n",
    "y_train2 = one_hot_encoding(y_train2)\n",
    "y_valid2 = one_hot_encoding(y_valid2)\n",
    "y_test2 = one_hot_encoding(y_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_only_softmax(X_train, y_train, X_validate, y_validate, train_op, epoch_bound, stop_threshold, batch_size, testing=False, new_saver=None, new_model_path=None):\n",
    "    \n",
    "    early_stop = 0\n",
    "    winner_loss = np.infty\n",
    "    winner_accuracy = 0.0\n",
    "    \n",
    "    t0 = time.time()\n",
    "    \n",
    "    for epoch in range(epoch_bound):\n",
    "\n",
    "        # randomize training set\n",
    "        indices_training = np.random.permutation(X_train.shape[0])\n",
    "        X_train, y_train = X_train[indices_training,:], y_train[indices_training,:]\n",
    "\n",
    "        # split training set into multiple mini-batches and start training\n",
    "        total_batches = int(X_train.shape[0] / batch_size)\n",
    "        for batch in range(total_batches):\n",
    "            if batch == total_batches - 1:\n",
    "                sess.run(train_op, feed_dict={X: X_train[batch*batch_size:], y: y_train[batch*batch_size:], mode:'TRAIN'})\n",
    "            else:\n",
    "                sess.run(train_op, feed_dict={X: X_train[batch*batch_size : (batch+1)*batch_size], y: y_train[batch*batch_size : (batch+1)*batch_size], mode:'TRAIN'})\n",
    "\n",
    "        # compute validation accuracy\n",
    "        cur_accuracy, cur_loss = evaluate_with_only_softmax(X_validate, y_validate)\n",
    "\n",
    "        # If the accuracy rate does not increase for many times, it will early stop epochs-loop \n",
    "        if winner_loss > cur_loss:\n",
    "            early_stop = 0\n",
    "            winner_loss = cur_loss\n",
    "            winner_accuracy = cur_accuracy\n",
    "            # save best model in testing phase\n",
    "            if testing == True:\n",
    "                save_path = new_saver.save(sess, new_model_path + \".ckpt\")\n",
    "        else:\n",
    "            early_stop += 1\n",
    "        print(\"{}\\tValidation loss: {:.6f}\\tBest loss: {:.6f}\\tAccuracy: {:.2f}%\".format(epoch, cur_loss, winner_loss, cur_accuracy * 100))\n",
    "        if early_stop == stop_threshold:\n",
    "            print(\"Early Stop.\")\n",
    "            break\n",
    "    t1 = time.time()\n",
    "    print(\"Total training time of HW3-1: {:.1f}s\".format(t1 - t0))\n",
    "    \n",
    "    return winner_accuracy, winner_loss\n",
    "\n",
    "# evaluate model: compute accuracy, precision, recall\n",
    "def evaluate_with_only_softmax(Inputs, Labels):\n",
    "    global Y_probability, loss\n",
    "    y_predict = sess.run(Y_probability, feed_dict={X: Inputs, y: Labels, mode:'EVAL'})\n",
    "    correct_prediction = tf.equal(tf.argmax(y_predict, 1), tf.argmax(Labels, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name=\"accuracy\")\n",
    "    acc = sess.run(accuracy, feed_dict={X: Inputs, y:Labels, mode:'EVAL'})\n",
    "    loss_val = sess.run(loss, feed_dict={X: Inputs, y:Labels, mode:'EVAL'})        \n",
    "    return acc, loss_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3-1: Softmax Only\n",
    "reset_graph()\n",
    "pretrained_model_path = \"./saved_model/Team35_HW2\"\n",
    "new_model_path = \"./saved_model/Team35_HW3_1\"\n",
    "pretrained_saver = tf.train.import_meta_graph(pretrained_model_path + \".ckpt.meta\")\n",
    "new_saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyper-parameters\n",
    "learning_rate = 0.01\n",
    "batch_size = 64\n",
    "epoch_bound = 1000\n",
    "stop_threshold = 20\n",
    "\n",
    "# 取得要transfer的圖 get graph for 1~5 layers (transfer layers) \n",
    "X = tf.get_default_graph().get_tensor_by_name(\"X:0\")\n",
    "y = tf.get_default_graph().get_tensor_by_name(\"y:0\")\n",
    "mode = tf.get_default_graph().get_tensor_by_name(\"mode:0\")\n",
    "loss = tf.get_default_graph().get_tensor_by_name(\"loss:0\")\n",
    "Y_probability = tf.get_default_graph().get_tensor_by_name(\"Y_probability:0\")\n",
    "logits = Y_probability.op.inputs[0]\n",
    "accuracy = tf.get_default_graph().get_tensor_by_name(\"accuracy:0\")\n",
    "\n",
    "# create new training layers\n",
    "output_layer_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"logits\")\n",
    "\n",
    "# define new training steps\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate, name=\"AdamOp_3-1\")\n",
    "training_op = optimizer.minimize(loss, var_list=output_layer_vars)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\tValidation loss: 192.551804\tBest loss: 192.551804\tAccuracy: 46.00%\n",
      "1\tValidation loss: 153.083832\tBest loss: 153.083832\tAccuracy: 60.67%\n",
      "2\tValidation loss: 132.998337\tBest loss: 132.998337\tAccuracy: 63.33%\n",
      "3\tValidation loss: 119.425430\tBest loss: 119.425430\tAccuracy: 73.33%\n",
      "4\tValidation loss: 110.371307\tBest loss: 110.371307\tAccuracy: 74.00%\n",
      "5\tValidation loss: 106.274345\tBest loss: 106.274345\tAccuracy: 75.33%\n",
      "6\tValidation loss: 101.357330\tBest loss: 101.357330\tAccuracy: 76.67%\n",
      "7\tValidation loss: 99.859924\tBest loss: 99.859924\tAccuracy: 75.33%\n",
      "8\tValidation loss: 96.798958\tBest loss: 96.798958\tAccuracy: 78.67%\n",
      "9\tValidation loss: 95.876167\tBest loss: 95.876167\tAccuracy: 76.67%\n",
      "10\tValidation loss: 93.794830\tBest loss: 93.794830\tAccuracy: 80.67%\n",
      "11\tValidation loss: 92.263268\tBest loss: 92.263268\tAccuracy: 78.67%\n",
      "12\tValidation loss: 91.889305\tBest loss: 91.889305\tAccuracy: 80.00%\n",
      "13\tValidation loss: 91.156952\tBest loss: 91.156952\tAccuracy: 78.67%\n",
      "14\tValidation loss: 89.887634\tBest loss: 89.887634\tAccuracy: 78.67%\n",
      "15\tValidation loss: 88.498627\tBest loss: 88.498627\tAccuracy: 80.00%\n",
      "16\tValidation loss: 88.117630\tBest loss: 88.117630\tAccuracy: 80.00%\n",
      "17\tValidation loss: 87.410652\tBest loss: 87.410652\tAccuracy: 80.67%\n",
      "18\tValidation loss: 86.346642\tBest loss: 86.346642\tAccuracy: 80.67%\n",
      "19\tValidation loss: 86.677277\tBest loss: 86.346642\tAccuracy: 80.67%\n",
      "20\tValidation loss: 85.581596\tBest loss: 85.581596\tAccuracy: 80.67%\n",
      "21\tValidation loss: 85.820213\tBest loss: 85.581596\tAccuracy: 80.67%\n",
      "22\tValidation loss: 84.179741\tBest loss: 84.179741\tAccuracy: 82.00%\n",
      "23\tValidation loss: 83.919647\tBest loss: 83.919647\tAccuracy: 81.33%\n",
      "24\tValidation loss: 83.805313\tBest loss: 83.805313\tAccuracy: 80.67%\n",
      "25\tValidation loss: 83.859894\tBest loss: 83.805313\tAccuracy: 81.33%\n",
      "26\tValidation loss: 83.334564\tBest loss: 83.334564\tAccuracy: 80.67%\n",
      "27\tValidation loss: 82.889320\tBest loss: 82.889320\tAccuracy: 81.33%\n",
      "28\tValidation loss: 82.223625\tBest loss: 82.223625\tAccuracy: 81.33%\n",
      "29\tValidation loss: 82.619476\tBest loss: 82.223625\tAccuracy: 81.33%\n",
      "30\tValidation loss: 82.086700\tBest loss: 82.086700\tAccuracy: 80.67%\n",
      "31\tValidation loss: 81.724854\tBest loss: 81.724854\tAccuracy: 81.33%\n",
      "32\tValidation loss: 82.838028\tBest loss: 81.724854\tAccuracy: 80.00%\n",
      "33\tValidation loss: 81.123535\tBest loss: 81.123535\tAccuracy: 81.33%\n",
      "34\tValidation loss: 81.192741\tBest loss: 81.123535\tAccuracy: 82.67%\n",
      "35\tValidation loss: 81.979065\tBest loss: 81.123535\tAccuracy: 82.00%\n",
      "36\tValidation loss: 80.979103\tBest loss: 80.979103\tAccuracy: 82.67%\n",
      "37\tValidation loss: 83.991821\tBest loss: 80.979103\tAccuracy: 80.67%\n",
      "38\tValidation loss: 81.289047\tBest loss: 80.979103\tAccuracy: 84.00%\n",
      "39\tValidation loss: 82.583351\tBest loss: 80.979103\tAccuracy: 81.33%\n",
      "40\tValidation loss: 80.730988\tBest loss: 80.730988\tAccuracy: 83.33%\n",
      "41\tValidation loss: 80.578171\tBest loss: 80.578171\tAccuracy: 82.67%\n",
      "42\tValidation loss: 80.518600\tBest loss: 80.518600\tAccuracy: 81.33%\n",
      "43\tValidation loss: 81.446030\tBest loss: 80.518600\tAccuracy: 81.33%\n",
      "44\tValidation loss: 81.960182\tBest loss: 80.518600\tAccuracy: 81.33%\n",
      "45\tValidation loss: 80.702911\tBest loss: 80.518600\tAccuracy: 82.00%\n",
      "46\tValidation loss: 80.760017\tBest loss: 80.518600\tAccuracy: 81.33%\n",
      "47\tValidation loss: 80.484322\tBest loss: 80.484322\tAccuracy: 82.67%\n",
      "48\tValidation loss: 79.710068\tBest loss: 79.710068\tAccuracy: 81.33%\n",
      "49\tValidation loss: 82.364555\tBest loss: 79.710068\tAccuracy: 80.67%\n",
      "50\tValidation loss: 81.674881\tBest loss: 79.710068\tAccuracy: 81.33%\n",
      "51\tValidation loss: 81.339836\tBest loss: 79.710068\tAccuracy: 81.33%\n",
      "52\tValidation loss: 80.670250\tBest loss: 79.710068\tAccuracy: 81.33%\n",
      "53\tValidation loss: 81.057884\tBest loss: 79.710068\tAccuracy: 82.00%\n",
      "54\tValidation loss: 82.218330\tBest loss: 79.710068\tAccuracy: 81.33%\n",
      "55\tValidation loss: 81.992149\tBest loss: 79.710068\tAccuracy: 81.33%\n",
      "56\tValidation loss: 82.115433\tBest loss: 79.710068\tAccuracy: 80.67%\n",
      "57\tValidation loss: 81.287003\tBest loss: 79.710068\tAccuracy: 81.33%\n",
      "58\tValidation loss: 81.476555\tBest loss: 79.710068\tAccuracy: 81.33%\n",
      "59\tValidation loss: 81.617851\tBest loss: 79.710068\tAccuracy: 81.33%\n",
      "60\tValidation loss: 81.801003\tBest loss: 79.710068\tAccuracy: 82.00%\n",
      "61\tValidation loss: 81.768982\tBest loss: 79.710068\tAccuracy: 81.33%\n",
      "62\tValidation loss: 82.003105\tBest loss: 79.710068\tAccuracy: 82.00%\n",
      "63\tValidation loss: 81.068710\tBest loss: 79.710068\tAccuracy: 82.00%\n",
      "64\tValidation loss: 81.847603\tBest loss: 79.710068\tAccuracy: 82.00%\n",
      "65\tValidation loss: 81.916794\tBest loss: 79.710068\tAccuracy: 81.33%\n",
      "66\tValidation loss: 82.107628\tBest loss: 79.710068\tAccuracy: 81.33%\n",
      "67\tValidation loss: 82.304245\tBest loss: 79.710068\tAccuracy: 82.67%\n",
      "68\tValidation loss: 82.108154\tBest loss: 79.710068\tAccuracy: 81.33%\n",
      "Early Stop.\n",
      "Total training time of HW3-1: 262.8s\n",
      "INFO:tensorflow:Restoring parameters from ./saved_model/Team35_HW3_1.ckpt\n",
      "Test accuracy: 80.85%\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    # init weights\n",
    "    sess.run(init)\n",
    "    \n",
    "    # 取得tenfor的數值 restore value of transfer layers\n",
    "    pretrained_saver.restore = (sess, pretrained_model_path + \".ckpt\")\n",
    "\n",
    "    # 初始新的layer initialize value for softmax layer\n",
    "    for var in output_layer_vars:\n",
    "        sess.run(var.initializer)\n",
    "    \n",
    "    winner_accuracy, winner_loss = train_with_only_softmax(X_train2, y_train2, X_valid2, y_valid2, training_op, epoch_bound, stop_threshold, batch_size, testing=True, new_saver=new_saver, new_model_path=new_model_path)\n",
    "\n",
    "    new_saver.restore(sess, new_model_path + \".ckpt\")\n",
    "    test_accuracy, test_loss = evaluate_with_only_softmax(X_test2, y_test2)\n",
    "    print(\"Test accuracy: {:.2f}%\".format(test_accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_softmax_and_cache_h5(X_train, y_train, X_validate, y_validate, train_op, epoch_bound, stop_threshold, batch_size, testing=False, new_saver=None, new_model_path=None):\n",
    "    \n",
    "    early_stop = 0\n",
    "    winner_loss = np.infty\n",
    "    winner_accuracy = 0.0\n",
    "    \n",
    "    t0 = time.time()\n",
    "    \n",
    "    for epoch in range(epoch_bound):\n",
    "\n",
    "        # randomize training set\n",
    "        indices_training = np.random.permutation(X_train.shape[0])\n",
    "        X_train, y_train = X_train[indices_training,:], y_train[indices_training,:]\n",
    "\n",
    "        # split training set into multiple mini-batches and start training\n",
    "        total_batches = int(X_train.shape[0] / batch_size)\n",
    "        for batch in range(total_batches):\n",
    "            if batch == total_batches - 1:\n",
    "                sess.run(train_op, feed_dict={h5_out: X_train[batch*batch_size:], y: y_train[batch*batch_size:], mode:'TRAIN'})\n",
    "            else:\n",
    "                sess.run(train_op, feed_dict={h5_out: X_train[batch*batch_size : (batch+1)*batch_size], y: y_train[batch*batch_size : (batch+1)*batch_size], mode:'TRAIN'})\n",
    "\n",
    "        # compute validation accuracy\n",
    "        cur_accuracy, cur_loss = evaluate_with_softmax_and_cache_h5(X_validate, y_validate)\n",
    "\n",
    "        # If the accuracy rate does not increase for many times, it will early stop epochs-loop \n",
    "        if winner_loss > cur_loss:\n",
    "            early_stop = 0\n",
    "            winner_loss = cur_loss\n",
    "            winner_accuracy = cur_accuracy\n",
    "            # save best model in testing phase\n",
    "            if testing == True:\n",
    "                save_path = new_saver.save(sess, new_model_path + \".ckpt\")\n",
    "        else:\n",
    "            early_stop += 1\n",
    "        print(\"{}\\tValidation loss: {:.6f}\\tBest loss: {:.6f}\\tAccuracy: {:.2f}%\".format(epoch, cur_loss, winner_loss, cur_accuracy * 100))\n",
    "        if early_stop == stop_threshold:\n",
    "            print(\"Early Stop.\")\n",
    "            break\n",
    "    t1 = time.time()\n",
    "    print(\"Total training time of HW3-1: {:.1f}s\".format(t1 - t0))\n",
    "    \n",
    "    return winner_accuracy, winner_loss\n",
    "\n",
    "# evaluate model: compute accuracy, precision, recall\n",
    "def evaluate_with_softmax_and_cache_h5(Inputs, Labels):\n",
    "    global Y_probability, loss\n",
    "    y_predict = sess.run(Y_probability, feed_dict={h5_out: Inputs, y: Labels, mode:'EVAL'})\n",
    "    correct_prediction = tf.equal(tf.argmax(y_predict, 1), tf.argmax(Labels, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name=\"accuracy\")\n",
    "    acc = sess.run(accuracy, feed_dict={h5_out: Inputs, y:Labels, mode:'EVAL'})\n",
    "    loss_val = sess.run(loss, feed_dict={h5_out: Inputs, y:Labels, mode:'EVAL'})        \n",
    "    return acc, loss_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the function, tf.get_default_graph().get_operations(), to find right tensor name\n",
    "#print(tf.get_default_graph().get_operations())\n",
    "h5_dropout = tf.get_default_graph().get_tensor_by_name(\"dropout5/Identity:0\")\n",
    "h5_out = h5_dropout.op.inputs[0]\n",
    "\n",
    "new_model_path = \"./saved_model/Team35_HW3_2\"\n",
    "new_saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\tValidation loss: 186.019379\tBest loss: 186.019379\tAccuracy: 50.00%\n",
      "1\tValidation loss: 149.375626\tBest loss: 149.375626\tAccuracy: 61.33%\n",
      "2\tValidation loss: 129.953323\tBest loss: 129.953323\tAccuracy: 67.33%\n",
      "3\tValidation loss: 118.137283\tBest loss: 118.137283\tAccuracy: 72.00%\n",
      "4\tValidation loss: 110.139992\tBest loss: 110.139992\tAccuracy: 70.67%\n",
      "5\tValidation loss: 104.321075\tBest loss: 104.321075\tAccuracy: 74.00%\n",
      "6\tValidation loss: 100.331009\tBest loss: 100.331009\tAccuracy: 76.67%\n",
      "7\tValidation loss: 98.222519\tBest loss: 98.222519\tAccuracy: 77.33%\n",
      "8\tValidation loss: 97.048592\tBest loss: 97.048592\tAccuracy: 77.33%\n",
      "9\tValidation loss: 94.297844\tBest loss: 94.297844\tAccuracy: 79.33%\n",
      "10\tValidation loss: 93.406281\tBest loss: 93.406281\tAccuracy: 76.67%\n",
      "11\tValidation loss: 91.472527\tBest loss: 91.472527\tAccuracy: 78.67%\n",
      "12\tValidation loss: 90.616837\tBest loss: 90.616837\tAccuracy: 77.33%\n",
      "13\tValidation loss: 90.367195\tBest loss: 90.367195\tAccuracy: 78.00%\n",
      "14\tValidation loss: 89.149986\tBest loss: 89.149986\tAccuracy: 79.33%\n",
      "15\tValidation loss: 87.848854\tBest loss: 87.848854\tAccuracy: 80.00%\n",
      "16\tValidation loss: 88.323746\tBest loss: 87.848854\tAccuracy: 79.33%\n",
      "17\tValidation loss: 87.012108\tBest loss: 87.012108\tAccuracy: 80.00%\n",
      "18\tValidation loss: 86.863686\tBest loss: 86.863686\tAccuracy: 81.33%\n",
      "19\tValidation loss: 85.760345\tBest loss: 85.760345\tAccuracy: 81.33%\n",
      "20\tValidation loss: 84.671829\tBest loss: 84.671829\tAccuracy: 80.67%\n",
      "21\tValidation loss: 84.041069\tBest loss: 84.041069\tAccuracy: 80.67%\n",
      "22\tValidation loss: 84.752579\tBest loss: 84.041069\tAccuracy: 82.00%\n",
      "23\tValidation loss: 83.984779\tBest loss: 83.984779\tAccuracy: 82.00%\n",
      "24\tValidation loss: 83.342674\tBest loss: 83.342674\tAccuracy: 81.33%\n",
      "25\tValidation loss: 83.009644\tBest loss: 83.009644\tAccuracy: 82.00%\n",
      "26\tValidation loss: 83.067917\tBest loss: 83.009644\tAccuracy: 82.67%\n",
      "27\tValidation loss: 82.837013\tBest loss: 82.837013\tAccuracy: 81.33%\n",
      "28\tValidation loss: 82.527229\tBest loss: 82.527229\tAccuracy: 81.33%\n",
      "29\tValidation loss: 82.718201\tBest loss: 82.527229\tAccuracy: 82.00%\n",
      "30\tValidation loss: 82.125412\tBest loss: 82.125412\tAccuracy: 83.33%\n",
      "31\tValidation loss: 81.515930\tBest loss: 81.515930\tAccuracy: 81.33%\n",
      "32\tValidation loss: 82.810120\tBest loss: 81.515930\tAccuracy: 81.33%\n",
      "33\tValidation loss: 81.292664\tBest loss: 81.292664\tAccuracy: 82.00%\n",
      "34\tValidation loss: 82.324127\tBest loss: 81.292664\tAccuracy: 80.00%\n",
      "35\tValidation loss: 82.258286\tBest loss: 81.292664\tAccuracy: 82.67%\n",
      "36\tValidation loss: 81.274666\tBest loss: 81.274666\tAccuracy: 81.33%\n",
      "37\tValidation loss: 82.053970\tBest loss: 81.274666\tAccuracy: 81.33%\n",
      "38\tValidation loss: 81.551750\tBest loss: 81.274666\tAccuracy: 80.00%\n",
      "39\tValidation loss: 81.406021\tBest loss: 81.274666\tAccuracy: 81.33%\n",
      "40\tValidation loss: 81.929680\tBest loss: 81.274666\tAccuracy: 82.67%\n",
      "41\tValidation loss: 81.812691\tBest loss: 81.274666\tAccuracy: 80.67%\n",
      "42\tValidation loss: 81.637238\tBest loss: 81.274666\tAccuracy: 82.00%\n",
      "43\tValidation loss: 81.884705\tBest loss: 81.274666\tAccuracy: 81.33%\n",
      "44\tValidation loss: 81.424362\tBest loss: 81.274666\tAccuracy: 82.67%\n",
      "45\tValidation loss: 81.336838\tBest loss: 81.274666\tAccuracy: 81.33%\n",
      "46\tValidation loss: 81.074020\tBest loss: 81.074020\tAccuracy: 81.33%\n",
      "47\tValidation loss: 81.377136\tBest loss: 81.074020\tAccuracy: 82.00%\n",
      "48\tValidation loss: 81.608353\tBest loss: 81.074020\tAccuracy: 81.33%\n",
      "49\tValidation loss: 81.652443\tBest loss: 81.074020\tAccuracy: 82.00%\n",
      "50\tValidation loss: 80.306320\tBest loss: 80.306320\tAccuracy: 82.67%\n",
      "51\tValidation loss: 81.854233\tBest loss: 80.306320\tAccuracy: 81.33%\n",
      "52\tValidation loss: 80.848595\tBest loss: 80.306320\tAccuracy: 82.00%\n",
      "53\tValidation loss: 82.217033\tBest loss: 80.306320\tAccuracy: 82.00%\n",
      "54\tValidation loss: 81.257980\tBest loss: 80.306320\tAccuracy: 81.33%\n",
      "55\tValidation loss: 81.200623\tBest loss: 80.306320\tAccuracy: 82.00%\n",
      "56\tValidation loss: 81.757103\tBest loss: 80.306320\tAccuracy: 82.00%\n",
      "57\tValidation loss: 82.640320\tBest loss: 80.306320\tAccuracy: 80.67%\n",
      "58\tValidation loss: 81.207077\tBest loss: 80.306320\tAccuracy: 82.00%\n",
      "59\tValidation loss: 82.434708\tBest loss: 80.306320\tAccuracy: 81.33%\n",
      "60\tValidation loss: 82.750000\tBest loss: 80.306320\tAccuracy: 81.33%\n",
      "61\tValidation loss: 81.911758\tBest loss: 80.306320\tAccuracy: 82.00%\n",
      "62\tValidation loss: 82.227287\tBest loss: 80.306320\tAccuracy: 81.33%\n",
      "63\tValidation loss: 82.299316\tBest loss: 80.306320\tAccuracy: 81.33%\n",
      "64\tValidation loss: 81.178909\tBest loss: 80.306320\tAccuracy: 82.00%\n",
      "65\tValidation loss: 84.381546\tBest loss: 80.306320\tAccuracy: 80.00%\n",
      "66\tValidation loss: 81.716339\tBest loss: 80.306320\tAccuracy: 82.00%\n",
      "67\tValidation loss: 82.928162\tBest loss: 80.306320\tAccuracy: 82.00%\n",
      "68\tValidation loss: 84.141418\tBest loss: 80.306320\tAccuracy: 81.33%\n",
      "69\tValidation loss: 81.284225\tBest loss: 80.306320\tAccuracy: 82.00%\n",
      "70\tValidation loss: 83.621826\tBest loss: 80.306320\tAccuracy: 80.67%\n",
      "Early Stop.\n",
      "Total training time of HW3-1: 236.2s\n",
      "INFO:tensorflow:Restoring parameters from ./saved_model/Team35_HW3_2.ckpt\n",
      "Test accuracy: 80.64%\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    # init weights\n",
    "    sess.run(init)\n",
    "    \n",
    "    # 取得tenfor的數值 restore value of transfer layers\n",
    "    pretrained_saver.restore = (sess, pretrained_model_path + \".ckpt\")\n",
    "\n",
    "    # 初始新的layer initialize value for softmax layer\n",
    "    for var in output_layer_vars:\n",
    "        sess.run(var.initializer)\n",
    "    \n",
    "    h5_train = sess.run(h5_out, feed_dict={X: X_train2, y: y_train2, mode:'TRAIN'})\n",
    "    h5_valid = sess.run(h5_out, feed_dict={X: X_valid2, y: y_valid2, mode:'TRAIN'})\n",
    "    \n",
    "    # training phase\n",
    "    winner_accuracy, winner_loss = train_with_softmax_and_cache_h5(h5_train, y_train2, h5_valid, y_valid2, training_op, epoch_bound, stop_threshold, batch_size, testing=True, new_saver=new_saver, new_model_path=new_model_path)\n",
    "\n",
    "    # testing phase\n",
    "    new_saver.restore(sess, new_model_path + \".ckpt\")\n",
    "    test_accuracy, test_loss = evaluate_with_only_softmax(X_test2, y_test2)\n",
    "    print(\"Test accuracy: {:.2f}%\".format(test_accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
