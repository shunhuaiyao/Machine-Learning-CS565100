{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "# to make sure the graph is refresh\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "reset_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# load data: digits 5 to 9, but still label with 0 to 4, \n",
    "# because TensorFlow expects label's integers from 0 to n_classes-1.\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")\n",
    "\n",
    "X_train2_full = mnist.train.images[mnist.train.labels >= 5]\n",
    "y_train2_full = mnist.train.labels[mnist.train.labels >= 5] - 5\n",
    "X_valid2_full = mnist.validation.images[mnist.validation.labels >= 5]\n",
    "y_valid2_full = mnist.validation.labels[mnist.validation.labels >= 5] - 5\n",
    "X_test2 = mnist.test.images[mnist.test.labels >= 5]\n",
    "y_test2 = mnist.test.labels[mnist.test.labels >= 5] - 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want to keep only 100 instances per class in the training set \n",
    "# and let's keep only 30 instances per class in the validation set\n",
    "# tesing set is already loaded above\n",
    "def sample_n_instances_per_class(X, y, n=100):\n",
    "    Xs, ys = [], []\n",
    "    for label in np.unique(y):\n",
    "        idx = (y == label)\n",
    "        Xc = X[idx][:n]\n",
    "        yc = y[idx][:n]\n",
    "        Xs.append(Xc)\n",
    "        ys.append(yc)\n",
    "    return np.concatenate(Xs), np.concatenate(ys)\n",
    "\n",
    "X_train2, y_train2 = sample_n_instances_per_class(X_train2_full, y_train2_full, n=100)\n",
    "X_valid2, y_valid2 = sample_n_instances_per_class(X_valid2_full, y_valid2_full, n=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one_hot encoding 5, 6, 7, 8, 9 for all labels\n",
    "def one_hot_encoding(y):\n",
    "    tmp_y = np.zeros([y.shape[0], 5])\n",
    "    for i in range(y.shape[0]):\n",
    "        tmp_y[i][y[i]] = 1\n",
    "    return tmp_y\n",
    "\n",
    "y_train2 = one_hot_encoding(y_train2)\n",
    "y_valid2 = one_hot_encoding(y_valid2)\n",
    "y_test2 = one_hot_encoding(y_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X_train, y_train, X_validate, y_validate, train_op, epoch_bound, stop_threshold, batch_size, testing=False, new_saver=None, new_model_path=None):\n",
    "    \n",
    "    early_stop = 0\n",
    "    winner_loss = np.infty\n",
    "    winner_accuracy = 0.0\n",
    "    \n",
    "    t0 = time.time()\n",
    "    \n",
    "    for epoch in range(epoch_bound):\n",
    "\n",
    "        # randomize training set\n",
    "        indices_training = np.random.permutation(X_train.shape[0])\n",
    "        X_train, y_train = X_train[indices_training,:], y_train[indices_training,:]\n",
    "\n",
    "        # split training set into multiple mini-batches and start training\n",
    "        total_batches = int(X_train.shape[0] / batch_size)\n",
    "        for batch in range(total_batches):\n",
    "            if batch == total_batches - 1:\n",
    "                sess.run(train_op, feed_dict={X: X_train[batch*batch_size:], y: y_train[batch*batch_size:], mode:'TRAIN'})\n",
    "            else:\n",
    "                sess.run(train_op, feed_dict={X: X_train[batch*batch_size : (batch+1)*batch_size], y: y_train[batch*batch_size : (batch+1)*batch_size], mode:'TRAIN'})\n",
    "\n",
    "        # compute validation accuracy\n",
    "        cur_accuracy, cur_loss = evaluate(X_validate, y_validate)\n",
    "\n",
    "        # If the accuracy rate does not increase for many times, it will early stop epochs-loop \n",
    "        if winner_loss > cur_loss:\n",
    "            early_stop = 0\n",
    "            winner_loss = cur_loss\n",
    "            winner_accuracy = cur_accuracy\n",
    "            # save best model in testing phase\n",
    "            if testing == True:\n",
    "                save_path = new_saver.save(sess, new_model_path + \".ckpt\")\n",
    "        else:\n",
    "            early_stop += 1\n",
    "        print(\"{}\\tValidation loss: {:.6f}\\tBest loss: {:.6f}\\tAccuracy: {:.2f}%\".format(epoch, cur_loss, winner_loss, cur_accuracy * 100))\n",
    "        if early_stop == stop_threshold:\n",
    "            print(\"Early Stop.\")\n",
    "            break\n",
    "    t1 = time.time()\n",
    "    print(\"Total training time of HW3-1: {:.1f}s\".format(t1 - t0))\n",
    "    \n",
    "    return winner_accuracy, winner_loss\n",
    "\n",
    "# evaluate model: compute accuracy, precision, recall\n",
    "def evaluate(Inputs, Labels):\n",
    "    global Y_probability, loss\n",
    "    y_predict = sess.run(Y_probability, feed_dict={X: Inputs, y: Labels, mode:'EVAL'})\n",
    "    correct_prediction = tf.equal(tf.argmax(y_predict, 1), tf.argmax(Labels, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name=\"accuracy\")\n",
    "    acc = sess.run(accuracy, feed_dict={X: Inputs, y:Labels, mode:'EVAL'})\n",
    "    loss_val = sess.run(loss, feed_dict={X: Inputs, y:Labels, mode:'EVAL'})        \n",
    "    return acc, loss_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3-1: Softmax Only\n",
    "reset_graph()\n",
    "pretrained_model_path = \"./saved_model/Team35_HW2\"\n",
    "new_model_path = \"./saved_model/Team35_HW3_1\"\n",
    "pretrained_saver = tf.train.import_meta_graph(pretrained_model_path + \".ckpt.meta\")\n",
    "new_saver = tf.train.Saver()\n",
    "\n",
    "# define hyper-parameters\n",
    "learning_rate = 0.01\n",
    "batch_size = 64\n",
    "epoch_bound = 1000\n",
    "stop_threshold = 20\n",
    "\n",
    "# 取得要transfer的圖 get graph for 1~5 layers (transfer layers) \n",
    "X = tf.get_default_graph().get_tensor_by_name(\"X:0\")\n",
    "y = tf.get_default_graph().get_tensor_by_name(\"y:0\")\n",
    "mode = tf.get_default_graph().get_tensor_by_name(\"mode:0\")\n",
    "loss = tf.get_default_graph().get_tensor_by_name(\"loss:0\")\n",
    "Y_probability = tf.get_default_graph().get_tensor_by_name(\"Y_probability:0\")\n",
    "logits = Y_probability.op.inputs[0]\n",
    "accuracy = tf.get_default_graph().get_tensor_by_name(\"accuracy:0\")\n",
    "\n",
    "# create new training layers\n",
    "output_layer_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"logits\")\n",
    "\n",
    "# define new training steps\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate, name=\"AdamOp_3-1\")\n",
    "training_op = optimizer.minimize(loss, var_list=output_layer_vars)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\tValidation loss: 192.551804\tBest loss: 192.551804\tAccuracy: 46.00%\n",
      "1\tValidation loss: 153.083847\tBest loss: 153.083847\tAccuracy: 60.67%\n",
      "2\tValidation loss: 132.998337\tBest loss: 132.998337\tAccuracy: 63.33%\n",
      "3\tValidation loss: 119.425430\tBest loss: 119.425430\tAccuracy: 73.33%\n",
      "4\tValidation loss: 110.371307\tBest loss: 110.371307\tAccuracy: 74.00%\n",
      "5\tValidation loss: 106.274345\tBest loss: 106.274345\tAccuracy: 75.33%\n",
      "6\tValidation loss: 101.357323\tBest loss: 101.357323\tAccuracy: 76.67%\n",
      "7\tValidation loss: 99.859924\tBest loss: 99.859924\tAccuracy: 75.33%\n",
      "8\tValidation loss: 96.798958\tBest loss: 96.798958\tAccuracy: 78.67%\n",
      "9\tValidation loss: 95.876167\tBest loss: 95.876167\tAccuracy: 76.67%\n",
      "10\tValidation loss: 93.794830\tBest loss: 93.794830\tAccuracy: 80.67%\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    # init weights\n",
    "    sess.run(init)\n",
    "    \n",
    "    # 取得tenfor的數值 restore value of transfer layers\n",
    "    pretrained_saver.restore = (sess, pretrained_model_path + \".ckpt\")\n",
    "\n",
    "    # 初始新的layer initialize value for softmax layer\n",
    "    for var in output_layer_vars:\n",
    "        sess.run(var.initializer)\n",
    "    \n",
    "    winner_accuracy, winner_loss = train(X_train2, y_train2, X_valid2, y_valid2, training_op, epoch_bound, stop_threshold, batch_size, testing=True, new_saver=new_saver, new_model_path=new_model_path)\n",
    "\n",
    "    new_saver.restore(sess, new_model_path + \".ckpt\")\n",
    "    test_accuracy, test_loss = evaluate(X_test2, y_test2)\n",
    "    print(\"Test accuracy: {:.2f}%\".format(test_accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretrained_model_path = \"./saved_model/Team35_HW2\"\n",
    "# pretrained_saver = tf.train.import_meta_graph(pretrained_model_path + \".ckpt.meta\")\n",
    "h5_out = tf.get_default_graph().get_tensor_by_name(\"dnn_h5:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    # init weights\n",
    "    sess.run(init)\n",
    "    \n",
    "    # 取得tenfor的數值 restore value of transfer layers\n",
    "    pretrained_saver.restore = (sess, pretrained_model_path + \".ckpt\")\n",
    "\n",
    "    # 初始新的layer initialize value for softmax layer\n",
    "    for var in output_layer_vars:\n",
    "        sess.run(var.initializer)\n",
    "    \n",
    "    h5_train = sess.run(h5_out, feed_dict={X: X_train2, y: y_train2, mode:'TRAIN'})\n",
    "    h5_valid = sess.run(h5_out, feed_dict={X: X_valid2, y: y_valid2, mode:'TRAIN'})\n",
    "    print(h5_train)\n",
    "    \n",
    "#     winner_accuracy, winner_loss = train(X_train2, y_train2, X_valid2, y_valid2, training_op, epoch_bound, stop_threshold, batch_size, testing=True, new_saver=new_saver, new_model_path=new_model_path)\n",
    "\n",
    "#     new_saver.restore(sess, new_model_path + \".ckpt\")\n",
    "#     test_accuracy, test_loss = evaluate(X_test2, y_test2)\n",
    "#     print(\"Test accuracy: {:.2f}%\".format(test_accuracy * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
